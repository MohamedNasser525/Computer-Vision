{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'KerasTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 187\u001b[0m\n\u001b[0;32m    183\u001b[0m num_patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m#  cnn => vit => model\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m vit_model \u001b[38;5;241m=\u001b[39m \u001b[43mVisionTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m vit_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m#################################################################################################3\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Flatten the output of the CNN\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 166\u001b[0m, in \u001b[0;36mVisionTransformer\u001b[1;34m(image_size, patch_size, num_classes, num_blocks, embed_dim, num_heads, ff_dim, rate)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Transformer Encoder\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_blocks):\n\u001b[1;32m--> 166\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Applying global average pooling to get a fixed-size representation\u001b[39;00m\n\u001b[0;32m    169\u001b[0m representation \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling1D()(embeddings)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'KerasTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from turtle import pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.applications import EfficientNetB0, preprocess_input\n",
    "# from tensorflow.keras.applications import EfficientNetB0, preprocess_input\n",
    "\n",
    "\n",
    "\n",
    "# Constants for LeNet-5 data preprocessing\n",
    "IMAGE_SIZE = (64, 64)  # LeNet-5 is designed for 32x32 grayscale images\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# Data preprocessing and augmentation using LeNet-5 approach\n",
    "train_datagen = ImageDataGenerator(\n",
    "# preprocessing_function=preprocess_input,\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.5, 1.5],  # Adjust brightness\n",
    "    channel_shift_range=50.0,  # Adjust channel intensity\n",
    "    vertical_flip=True,  # Flip vertically\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    zca_whitening=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Data preparation for LeNet-5\n",
    "def create_train_data():\n",
    "    original_count = 0\n",
    "    augmented_count = 0\n",
    "    training_data = []\n",
    "    for folders in tqdm(os.listdir('C:/Users/ALSHARKAOY/Desktop/dataset/dataset/train')):\n",
    "        num_of_folder = 'C:/Users/ALSHARKAOY/Desktop/dataset/dataset/train' + \"/\" + str(folders)\n",
    "        for img in tqdm(os.listdir(num_of_folder)):\n",
    "            original_count += 1  # Increment original count for each image\n",
    "            path = os.path.join(num_of_folder, img)\n",
    "            img_data = cv2.imread(path, 0)\n",
    "            img_data = cv2.resize(img_data, (IMAGE_SIZE[0], IMAGE_SIZE[1]))\n",
    "            img_data = img_data.reshape((1,) + img_data.shape + (1,))  # Reshape for augmentation\n",
    "\n",
    "            # Generate augmented images\n",
    "            i = 0\n",
    "            for batch in train_datagen.flow(img_data, batch_size=1):\n",
    "                augmented_count += 1  # Increment augmented count for each augmentation\n",
    "                image = batch[0].reshape(IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
    "                label = np.zeros(NUM_CLASSES)  # Create an array of zeros\n",
    "                label[int(folders) - 1] = 1  # Set the appropriate index to 1\n",
    "                training_data.append([np.array(image), label])\n",
    "                i += 1\n",
    "                if i >= 5:  # Number of augmented images per original image\n",
    "                    break\n",
    "\n",
    "    shuffle(training_data)\n",
    "    print(\"Original data count:\", original_count)\n",
    "    print(\"Augmented data count:\", augmented_count)\n",
    "    return training_data\n",
    "\n",
    "import joblib # save model with joblib\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "\n",
    "\n",
    "if (os.path.exists('train_data_cnn.npy')): # If you have already created the dataset:\n",
    "    train_data =joblib.load('train_data_cnn.npy')\n",
    "else: # If dataset is not created:\n",
    "    train_data = create_train_data()\n",
    "    joblib.dump(train_data, \"train_data_cnn.npy\")\n",
    "\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "train, test = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reshape and preprocess data for LeNet-5\n",
    "X_train = np.array([i[0] for i in train]).reshape(-1, IMAGE_SIZE[0], IMAGE_SIZE[1], 1)\n",
    "Y_train = np.array([i[1] for i in train])\n",
    "\n",
    "X_test = np.array([i[0] for i in test]).reshape(-1, IMAGE_SIZE[0], IMAGE_SIZE[1], 1)\n",
    "Y_test = np.array([i[1] for i in test])\n",
    "\n",
    "\n",
    "cnn_model = Sequential([\n",
    "\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "\n",
    "])\n",
    "import tensorflow as tf\n",
    "# from keras.applications import EfficientNetB0, preprocess_input\n",
    "\n",
    "# Load pre-trained ViT model\n",
    "# transformer_model = tf.keras.applications.EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "# from keras.layers import Patches\n",
    "\n",
    "def MultiHeadSelfAttention(embed_dim, num_heads):\n",
    "    return layers.MultiHeadAttention(\n",
    "        key_dim=embed_dim // num_heads,\n",
    "        num_heads=num_heads,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "def TransformerBlock(embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    inputs = tf.keras.Input(shape=(None, embed_dim))\n",
    "    attention = MultiHeadSelfAttention(embed_dim, num_heads)(inputs, inputs)\n",
    "    attention = layers.Dropout(rate)(attention)\n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(attention)\n",
    "    ff = layers.Dropout(rate)(ff)\n",
    "    ff = layers.Conv1D(filters=embed_dim, kernel_size=1)(ff)\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(attention + ff)\n",
    "\n",
    "def VisionTransformer(image_size, patch_size, num_classes, num_blocks, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    input_shape = (image_size, image_size, 3)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Reshape the input image into patches\n",
    "    patch_height = image_size // patch_size\n",
    "    patch_width = image_size // patch_size\n",
    "    patches = layers.Reshape((patch_height * patch_width, patch_size, patch_size, 3))(inputs)\n",
    "    patches = layers.Permute((1, 3, 2, 4))(patches)\n",
    "    patches = layers.Reshape((patch_height * patch_width, patch_size * patch_size * 3))(patches)\n",
    "\n",
    "    # Projection and Positional Embeddings\n",
    "    embeddings = layers.Dense(embed_dim, activation=\"linear\")(patches)\n",
    "    embeddings += tf.keras.Input(shape=(embed_dim,))\n",
    "\n",
    "    # Transformer Encoder\n",
    "    for _ in range(num_blocks):\n",
    "        embeddings = TransformerBlock(embed_dim, num_heads, ff_dim, rate)(embeddings)\n",
    "\n",
    "    # Applying global average pooling to get a fixed-size representation\n",
    "    representation = layers.GlobalAveragePooling1D()(embeddings)\n",
    "\n",
    "    # Classifier head\n",
    "    logits = layers.Dense(num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "# Example usage\n",
    "image_size = IMAGE_SIZE[0]\n",
    "patch_size = 16\n",
    "num_classes = 5\n",
    "num_blocks = 12\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "ff_dim = 3072\n",
    "num_patches = 16\n",
    "\n",
    "#  cnn => vit => model\n",
    "\n",
    "vit_model = VisionTransformer(image_size, patch_size, num_classes, num_blocks, embed_dim, num_heads, ff_dim)\n",
    "vit_model.summary()\n",
    "#################################################################################################3\n",
    "\n",
    "# Flatten the output of the CNN\n",
    "flattened_output = cnn_model.output\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a positional embedding layer\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, embedding_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=sequence_length, output_dim=embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
    "        positional_embeddings = self.embedding(position_indices)\n",
    "        return inputs + positional_embeddings\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "positional_embeddings = PositionalEmbedding(num_patches, embed_dim)(flattened_output)\n",
    "positional_embeddings = layers.Reshape((num_patches, embed_dim))(positional_embeddings)\n",
    "\n",
    "# Reshape the output of the CNN to (num_patches, embed_dim)\n",
    "reshape_output = layers.Reshape((num_patches, embed_dim))(flattened_output)\n",
    "\n",
    "# Concatenate or add positional embeddings to the CNN output\n",
    "combined_features = layers.Concatenate(axis=1)([reshape_output, positional_embeddings])\n",
    "# Reshape for ViT model input\n",
    "combined_features = layers.Reshape((-1, embed_dim))(combined_features)\n",
    "\n",
    "# Example ViT model\n",
    "transformer_output = vit_model(combined_features)\n",
    "\n",
    "\n",
    "combined_model = tf.keras.Model(inputs=cnn_model.input, outputs=transformer_output)\n",
    "\n",
    "combined_model.compile(optimizer='adam', loss='categorical_crossentropy',learning_rate=.001, metrics=['accuracy'])\n",
    "\n",
    "combined_model.fit(X_train, Y_train, batch_size=64, validation_data=(X_test, Y_test),\n",
    "                            epochs=5, verbose=1)\n",
    "\n",
    "\n",
    "# importing required module\n",
    "import csv\n",
    "\n",
    "# opening the file\n",
    "with open(\"final.csv\", \"w\", newline=\"\") as f:\n",
    "    # creating the writer\n",
    "    writer = csv.writer(f)\n",
    "    # using writerow to write individual record one by one\n",
    "    # writer.writerow([\"Image\", \"Label\", \"Index\"])\n",
    "    writer.writerow([\"image_id\", \"label\"])\n",
    "    # Make predictions\n",
    "    label = [\"apple\", \"banana\", \"grapes\", \"mango\", \"stra\"]\n",
    "    for image in tqdm(os.listdir('C:/Users/monasser/Desktop/nural_project/dataset/test')):\n",
    "        img = cv2.imread(os.path.join('C:/Users/monasser/Desktop/nural_project/dataset/test', image), 0)\n",
    "        img_test = cv2.resize(img, (IMAGE_SIZE))\n",
    "        img_test = img_test.reshape(1, IMAGE_SIZE[0],IMAGE_SIZE[1], 1)  # Reshape for model input\n",
    "        prediction = combined_model.predict(img_test)[0]\n",
    "        max_index = np.argmax(prediction)\n",
    "        print(image.split('.')[0], \"  \", label[max_index])\n",
    "        # writer.writerow([image.split('.')[0],  label[max_index], max_index+1])\n",
    "        writer.writerow([image.split('.')[0], max_index+1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
